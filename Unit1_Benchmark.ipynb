{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unit 1 Assignment: The Model Benchmark Challenge\n",
        "\n",
        "This notebook benchmarks BERT, RoBERTa, and BART on tasks that highlight architectural differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install/Import Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running in a fresh environment, install transformers\n",
        "# !pip install transformers --quiet\n",
        "\n",
        "from transformers import pipeline, set_seed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models to Test\n",
        "- **BERT** (`bert-base-uncased`) \u2013 Encoder-only\n",
        "- **RoBERTa** (`roberta-base`) \u2013 Encoder-only\n",
        "- **BART** (`facebook/bart-base`) \u2013 Encoder-decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Text Generation\n",
        "Prompt: **\"The future of Artificial Intelligence is\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "generation_prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "text_generation_models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\",\n",
        "}\n",
        "\n",
        "text_generation_results = {}\n",
        "\n",
        "for name, model_id in text_generation_models.items():\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model_id)\n",
        "        output = generator(generation_prompt, max_length=40, num_return_sequences=1)\n",
        "        text_generation_results[name] = output[0][\"generated_text\"]\n",
        "    except Exception as exc:\n",
        "        text_generation_results[name] = f\"Failed: {exc}\"\n",
        "\n",
        "text_generation_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hypothesis:** Encoder-only models (BERT, RoBERTa) should struggle with text generation because they are not trained for autoregressive decoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Masked Language Modeling (Missing Word)\n",
        "Sentence: **\"The goal of Generative AI is to [MASK] new content.\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fill_mask_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "\n",
        "mask_models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\",\n",
        "}\n",
        "\n",
        "fill_mask_results = {}\n",
        "\n",
        "for name, model_id in mask_models.items():\n",
        "    try:\n",
        "        masker = pipeline(\"fill-mask\", model=model_id)\n",
        "        masked_input = fill_mask_sentence.replace(\"[MASK]\", masker.tokenizer.mask_token)\n",
        "        output = masker(masked_input)\n",
        "        fill_mask_results[name] = output[0][\"token_str\"].strip()\n",
        "    except Exception as exc:\n",
        "        fill_mask_results[name] = f\"Failed: {exc}\"\n",
        "\n",
        "fill_mask_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hypothesis:** BERT and RoBERTa should perform well because they are trained with masked language modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Question Answering\n",
        "Question: **\"What are the risks?\"**\n",
        "Context: **\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_question = \"What are the risks?\"\n",
        "qa_context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "qa_models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\",\n",
        "}\n",
        "\n",
        "qa_results = {}\n",
        "\n",
        "for name, model_id in qa_models.items():\n",
        "    try:\n",
        "        qa_pipeline = pipeline(\"question-answering\", model=model_id)\n",
        "        output = qa_pipeline(question=qa_question, context=qa_context)\n",
        "        qa_results[name] = output.get(\"answer\")\n",
        "    except Exception as exc:\n",
        "        qa_results[name] = f\"Failed: {exc}\"\n",
        "\n",
        "qa_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** These base models are not fine-tuned for QA, so answers may be poor or random.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observation Table\n",
        "Fill in the outputs you observe after running each cell.\n",
        "\n",
        "| Experiment | BERT (bert-base-uncased) | RoBERTa (roberta-base) | BART (facebook/bart-base) | Observations |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| Text Generation |  |  |  |  |\n",
        "| Masked LM |  |  |  |  |\n",
        "| Question Answering |  |  |  |  |\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}