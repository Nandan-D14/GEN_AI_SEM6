{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandan-D14/GEN_AI_SEM6/blob/main/PES2UG23CS363_Unit1_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAME : NANDAN D\n",
        "## SRN : PES2UG23CS363\n",
        "## SEC : F"
      ],
      "metadata": {
        "id": "SHGovrs02s5w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B69dl5s7poci"
      },
      "source": [
        "# Unit 1 Hands-on: Generative AI & NLP Fundamentals\n",
        "\n",
        "Welcome to your interactive guide to **Generative AI**. This notebook is designed to be a step-by-step tutorial, explaining not just *how* to code, but *why* we use these tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lXHp5sbjpoc1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "VO1dZqYJpoc7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9ZXsBJfhpoc-"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/unit 1.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c3hVpwPpoc_",
        "outputId": "0b150a8b-0307-4bda-dd3d-6a9c0a921889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{file_path}' not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4mdao5ypodB",
        "outputId": "63ace0a6-678f-4077-bc80-9c9138115ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Preview ---\n",
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Data Preview ---\")\n",
        "print(text[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwLAKcdspodC"
      },
      "source": [
        "## 2. Generative AI: RoBERTa vs BART\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0pOJ95napodD"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5YLKUq46podF"
      },
      "outputs": [],
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "roberta = \"roberta-base\"\n",
        "bart = \"facebook/bart-base\"\n",
        "bert = \"bert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pipeline with the specific model\n",
        "fast_generator = pipeline('text-generation', model=roberta)\n",
        "\n",
        "output_fast = fast_generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh6WCYK9uTps",
        "outputId": "df54d42c-72ef-4b4d-c845-8b2ebfc88e82"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp5zlHfBpodG",
        "outputId": "d44c3ae3-1506-4810-ff97-c1807a9ddb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is Cosby238 phyl Bradford *** Scholarship Scholarship assailant238238 Cosby Ahmadename Nazi/// � 560 Ahmad dominant Ahmad Morocco Nazi appendix Cosby lbs Dani piled DaniDNA Nazi downgrade Bradford warranted commentator Nazi lbs layoffs Nazi boost Nazi Nazi Nazi boostzzoValues boost appendixomething�\n"
          ]
        }
      ],
      "source": [
        "# Initialize the pipeline with the specific model\n",
        "fast_generator = pipeline('text-generation', model= bart)\n",
        "\n",
        "# Generate text\n",
        "output_fast = fast_generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmHWLRrYpodH",
        "outputId": "1db2a278-5138-417a-8fb9-de8cfc34f453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
          ]
        }
      ],
      "source": [
        "smart_generator = pipeline('text-generation', model=bert)\n",
        "\n",
        "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_smart[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_637cP4podI"
      },
      "source": [
        "## 3. NLP Fundamentals: Under the Hood\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lnS31C6podI"
      },
      "source": [
        "### 3.1 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hivacchKpodJ"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize the Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "P5qPFjdIpodK"
      },
      "outputs": [],
      "source": [
        "sample_sentence = \"Transformers revolutionized NLP.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRqKGCBOpodL"
      },
      "source": [
        " split it into tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSCBX750podM",
        "outputId": "823c74f7-24f4-44b9-cf39-c3fea6593cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Transform', 'ers', 'Ġrevolution', 'ized', 'ĠN', 'LP', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(sample_sentence)\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5sg7pLpodQ"
      },
      "source": [
        "### 3.2 POS Tagging (Part-of-Speech)\n",
        "**Why?** To understand grammar. Is 'book' a noun (the object) or a verb (to book a flight)?\n",
        "**What?** We label each word as Noun (NN), Verb (VB), Adjective (JJ), etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEuiK7jepodU",
        "outputId": "9a1606e0-9fbc-4233-a85c-c86f2b73f258"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQOYkvESpodX",
        "outputId": "d742aedb-5759-4461-ac3b-ed9753df0029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab', quiet=True)\n",
        "pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
        "print(f\"POS Tags: {pos_tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV5kUOYGpodZ"
      },
      "source": [
        "### 3.3 Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25j0S2hSpodd",
        "outputId": "9c45d55e-217f-495b-8dab-b7fb76d14451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Initialize NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jED4vki8podg",
        "outputId": "addde27d-9b4d-4408-c39a-f39089286c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity               | Type       | Score\n",
            "---------------------------------------------\n",
            "AI                   | MISC       | 0.98\n",
            "PES University       | ORG        | 0.99\n",
            "AI                   | MISC       | 0.98\n",
            "Large Language Models | MISC       | 0.91\n",
            "LLMs                 | MISC       | 0.90\n",
            "Transformer          | MISC       | 0.99\n"
          ]
        }
      ],
      "source": [
        "snippet = text[:1000]\n",
        "entities = ner_pipeline(snippet)\n",
        "\n",
        "print(f\"{'Entity':<20} | {'Type':<10} | {'Score':<5}\")\n",
        "print(\"-\"*45)\n",
        "for entity in entities:\n",
        "    if entity['score'] > 0.90:\n",
        "        print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g17DfdCpodh"
      },
      "source": [
        "## 4. Advanced Applications: Comparative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "k2cDaK7Npodi"
      },
      "outputs": [],
      "source": [
        "# Let's extract a specific section for summarization\n",
        "transformer_section = \"\"\"\n",
        "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n",
        "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n",
        "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHGr9H8jpodk",
        "outputId": "81b3c07f-4c62-40de-b1e7-9ab63a9d664b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'SummarizationPipeline' object has no attribute 'assistant_model'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  fast_sum = pipeline(\"summarization\", model= roberta)\n",
        "  res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "  print(res_fast[0]['summary_text'])\n",
        "except Exception as error :\n",
        "  print(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UqEEZ2Hpodm",
        "outputId": "05175181-146a-4830-8114-2664439caf8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions. The Transformer was also the first to implement a multi-head attention mechanism in a neural network. The implementation of this architecture is called the \"transformer\" architecture.The fundamental innovation of theTransformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the output, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.The Transformer ArchitectureThe TransTransformer architecture consists of an encoder stack (to process the input) and a decoder stack that to generate the output), both of which heavily utilize multi-headed attention and feed-forward networks.The Architecture\n"
          ]
        }
      ],
      "source": [
        "smart_sum = pipeline(\"summarization\", model= bart)\n",
        "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "print(res_smart[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try :\n",
        "  smart_sum = pipeline(\"summarization\", model= bert)\n",
        "  res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "  print(res_smart[0]['summary_text'])\n",
        "except Exception as error :\n",
        "  print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06C3R4FVxvgR",
        "outputId": "cba01c03-5b84-4b1e-c44d-fbcbccfbe8f0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'SummarizationPipeline' object has no attribute 'assistant_model'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZty9epppodm"
      },
      "source": [
        "### 4.2 Question Answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcZTzARmpodn",
        "outputId": "2986dbcd-e57c-40a9-ac07-640c49eddae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "qa_pipeline1 = pipeline(\"question-answering\", model= roberta)\n",
        "qa_pipeline2 = pipeline(\"question-answering\", model= bart)\n",
        "qa_pipeline3 = pipeline(\"question-answering\", model= bert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYVdZUgopodo",
        "outputId": "d5587071-1f80-4b25-bbea-3035592ceb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " roberta-base\n",
            "\n",
            "Q: Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
            "A: composed of many layers (hence\n",
            "\n",
            " roberta-base\n",
            "\n",
            "Q: What are the risks?\n",
            "A: composed of many layers (hence\n",
            "\n",
            " facebook/bart-base\n",
            "\n",
            "Q: Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
            "A: -Speech (POS) tagging and Named Entity Recognition (NER\n",
            "\n",
            " facebook/bart-base\n",
            "\n",
            "Q: What are the risks?\n",
            "A: distribution of the data for each class (i.\n",
            "\n",
            " bert-base-uncased\n",
            "\n",
            "Q: Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
            "A: , the model is trained on labeled data, meaning each\n",
            "\n",
            " bert-base-uncased\n",
            "\n",
            "Q: What are the risks?\n",
            "A: , the model is trained on labeled data, meaning each\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\",\n",
        "    \"What are the risks?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline1(question=q, context=text[:5000])\n",
        "    print( '\\n', roberta ,)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n",
        "\n",
        "\n",
        "for q in questions:\n",
        "    print( '\\n',bart ,)\n",
        "    res = qa_pipeline2(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n",
        "\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline3(question=q, context=text[:5000])\n",
        "    print( '\\n',bert ,)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQgAYt7podq"
      },
      "source": [
        "### 4.3 Masked Language Modeling (The 'Fill-in-the-Blank' Game)\n",
        "\n",
        "This is the core training objective of BERT. We hide a token (`[MASK]`) and ask the model to predict it based on context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3ikY9S8podr",
        "outputId": "98d94018-cc54-4577-8820-492a8a5acfdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "mask_filler1 = pipeline('fill-mask', model= roberta)\n",
        "mask_filler2 = pipeline('fill-mask', model=bart)\n",
        "mask_filler3 = pipeline('fill-mask', model=bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUgMG0xXpodt",
        "outputId": "0ddea843-bdfd-4890-8c62-a72e322b84da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RoBERTa Predictions ---\n",
            " generate: 0.37\n",
            " create: 0.37\n",
            " discover: 0.08\n",
            " find: 0.02\n",
            " provide: 0.02\n",
            "\n",
            "--- BART Predictions ---\n",
            " create: 0.07\n",
            " help: 0.07\n",
            " provide: 0.06\n",
            " enable: 0.04\n",
            " improve: 0.03\n",
            "\n",
            "--- BERT Predictions ---\n",
            "create: 0.54\n",
            "generate: 0.16\n",
            "produce: 0.05\n",
            "develop: 0.04\n",
            "add: 0.02\n"
          ]
        }
      ],
      "source": [
        "masked_sentence_roberta_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "masked_sentence_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "\n",
        "preds1 = mask_filler1(masked_sentence_roberta_bart)\n",
        "preds2 = mask_filler2(masked_sentence_roberta_bart)\n",
        "preds3 = mask_filler3(masked_sentence_bert)\n",
        "\n",
        "print(\"--- RoBERTa Predictions ---\")\n",
        "for pred in preds1:\n",
        "    print(f\"{pred['token_str']}: {pred['score']:.2f}\")\n",
        "\n",
        "print(\"\\n--- BART Predictions ---\")\n",
        "for pred in preds2:\n",
        "    print(f\"{pred['token_str']}: {pred['score']:.2f}\")\n",
        "\n",
        "print(\"\\n--- BERT Predictions ---\")\n",
        "for pred in preds3:\n",
        "    print(f\"{pred['token_str']}: {pred['score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverable: Observation Table\n",
        "\n",
        "\n",
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|---------------------------------------|---------------------------------------------|\n",
        "| Generation | BERT | Failure | Generated a long sequence of `.` (degenerate output); warning about needing `is_decoder=True`. | BERT is an **encoder-only masked-LM**; it’s not trained for next-token generation, so text-generation behaves poorly. |\n",
        "|  | RoBERTa | Failure | Printed only the prompt (no meaningful continuation); warning about needing `is_decoder=True`. | RoBERTa is also an **encoder-only masked-LM**; without decoder-style setup, it can’t do fluent autoregressive generation. |\n",
        "|  | BART | Failure | Generated mostly gibberish tokens and odd characters; warning that CausalLM weights were newly initialized. | BART is **seq2seq (encoder-decoder)**; forcing it into `text-generation` (CausalLM) leads to mismatched / randomly initialized heads, so output becomes nonsense. |\n",
        "| Fill-Mask | BERT | Success | Predicted good verbs like `create`, `generate` with high confidence. | BERT is trained on **Masked Language Modeling (MLM)**, so fill-mask matches its objective. |\n",
        "|  | RoBERTa | Success | Predicted `generate`, `create`, etc. with reasonable scores. | RoBERTa is trained on **MLM**, so it performs well at masked-token prediction. |\n",
        "|  | BART | Success (Weak) | Returned reasonable words (`create`, `help`, `provide`) but with lower/confused scores. | BART is a **denoising seq2seq** model (good at infilling), but it’s not a pure masked-LM head; results can be less confident in this pipeline setup. |\n",
        "| QA | BERT | Failure | Answers were unrelated spans (e.g., fragments about “trained on labeled data”); QA head warned as newly initialized. | These are **base** checkpoints, not fine-tuned for QA; the QA head has random weights, so extraction is unreliable. |\n",
        "|  | RoBERTa | Failure | Produced irrelevant text spans; warnings about `qa_outputs` being newly initialized. | Same reason: not QA-fine-tuned; random QA head = incorrect spans. |\n",
        "|  | BART | Failure | Returned unrelated spans (e.g., pieces about POS/NER); warnings about newly initialized QA head. | Same reason: not QA-fine-tuned; QA head is not trained for this context-question task. |"
      ],
      "metadata": {
        "id": "L4KaVcjMAZ_M"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}