{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Nandan D | PES2UG23CS363 | F**\n",
        "## LangChain Assignment"
      ],
      "metadata": {
        "id": "w8n7EOjb29UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2 - Part 1a: LangChain Setup & Models\n"
      ],
      "metadata": {
        "id": "FlKd9mCBzJa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X90i_Lcwib8",
        "outputId": "3be3c80c-05e4-4360-a172-4c3efb3eb3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-google-genai langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI8fxHPKxzKa",
        "outputId": "327ce13c-9095-48ca-f092-23b70f118d96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Model A: The \"Accountant\" (Precision)\n",
        "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "\n",
        "# Model B: The \"Poet\" (Creativity)\n",
        "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
      ],
      "metadata": {
        "id": "WnoPN9YKx2ji"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'Idea' in one sentence.\"\n",
        "\n",
        "print(\"--- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHRHNkD_yTOz",
        "outputId": "d4e889b9-a296-420e-a66a-50e961ed1afb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a thought, concept, or mental image formed in the mind.\n",
            "Run 2: An idea is a thought, concept, or suggestion that is formed or exists in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- CREATIVE (Temp=1) ---\")\n",
        "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61p8jJD-yVQU",
        "outputId": "5b20e7ab-9bc5-4a9e-c4d9-1d90ed3c5343"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CREATIVE (Temp=1) ---\n",
            "Run 1: An idea is a mental concept, thought, or impression formed in the mind, often representing a plan, solution, or understanding.\n",
            "Run 2: An idea is a mental concept, thought, or impression formed in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2 - Part 1b: Prompts & Parsers"
      ],
      "metadata": {
        "id": "8pwNyrwzyrzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup from Part 1a (Hidden for brevity)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "yOyClPqTytkM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Scenario: Make the AI rude.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a rude teenager. You use slang and don't care about grammar.\"),\n",
        "    HumanMessage(content=\"What is the capital of France?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kI-8kc0y8eU",
        "outputId": "e0f6feb9-5093-41bd-8db2-bdbfd1eeb134"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ugh, like, seriously? You don't know that? It's **Paris**, duh. Get a life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# We can check what inputs it expects\n",
        "print(f\"Required variables: {template.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hs_tcr9zA1H",
        "outputId": "d4b6c91a-2504-4850-cc5a-2298c77e84bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required variables: ['input_language', 'output_language', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Raw Message\n",
        "raw_msg = llm.invoke(\"Hi\")\n",
        "print(f\"Raw Type: {type(raw_msg)}\")\n",
        "\n",
        "# Parsed String\n",
        "clean_text = parser.invoke(raw_msg)\n",
        "print(f\"Parsed Type: {type(clean_text)}\")\n",
        "print(f\"Content: {clean_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnNjFa0CzZ5a",
        "outputId": "3b05c19a-f38b-4604-d914-fde37bd84a6a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Parsed Type: <class 'langchain_core.messages.base.TextAccessor'>\n",
            "Content: Hi there! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1c: LCEL (LangChain Expression Language)"
      ],
      "metadata": {
        "id": "0upBjNzD2J1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup (Hidden)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "8W9inMni2DW3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Format inputs\n",
        "prompt_value = template.invoke({\"topic\": \"Crows\"})\n",
        "\n",
        "# Step 2: Call Model\n",
        "response_obj = llm.invoke(prompt_value)\n",
        "\n",
        "# Step 3: Parse Output\n",
        "final_text = parser.invoke(response_obj)\n",
        "\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSs3-xyZ2OY-",
        "outputId": "f5027253-6a30-4599-b0c3-07ab25aa3847"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about crows:\n",
            "\n",
            "Crows are incredibly intelligent and can recognize individual human faces! If they have a bad experience with someone (like being chased or threatened), they'll not only remember that person for years, but they might even teach other crows in their flock to recognize and avoid (or even scold!) that specific individual. So, it literally pays to be nice to crows!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chain once\n",
        "chain = template | llm | parser\n",
        "\n",
        "# Invoke the whole chain\n",
        "print(chain.invoke({\"topic\": \"Octopuses\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAFe58p72Qmn",
        "outputId": "3f1ba58b-b763-48fb-d6db-d57987059a5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about octopuses:\n",
            "\n",
            "Octopuses have **three hearts** and **blue blood**! Two hearts pump blood through their gills, and a third, larger heart circulates blood to the rest of their body. Their blood is blue because it uses a copper-based protein called hemocyanin to transport oxygen, instead of the iron-based hemoglobin that makes our blood red.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "\n",
        "Create a chain that:\n",
        "1.  Takes a movie name.\n",
        "2.  Asks for its release year.\n",
        "3.  Calculates how many years ago that was (You can try just asking the LLM to do the math).\n",
        "\n",
        "Try to do it in **one line of LCEL**.\n"
      ],
      "metadata": {
        "id": "nmMOGDTx2Xpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Create a ChatPromptTemplate as instructed, defining the system message and human message with a placeholder for 'movie_name' to find the release year and calculate years since release, assuming the current year is 2026.\n",
        "\n"
      ],
      "metadata": {
        "id": "f3geuJHG25af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "movie_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"For the movie \\\"{movie_name}\\\", state its release year and calculate how many years ago that was, assuming the current year is 2026.\"\n",
        ")\n",
        "\n",
        "print(\"ChatPromptTemplate for movie details created and assigned to movie_prompt_template.\")\n",
        "print(f\"Required input variables: {movie_prompt_template.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tady_U2U2YmB",
        "outputId": "c00e5395-d1a3-4452-c1cd-c3f9da606b8e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatPromptTemplate for movie details created and assigned to movie_prompt_template.\n",
            "Required input variables: ['movie_name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Combine the `movie_prompt_template`, `llm`, and `parser` into a single LCEL chain using the pipe operator and assign it to `movie_chain`.\n",
        "\n"
      ],
      "metadata": {
        "id": "xr4fDGf822NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_chain = movie_prompt_template | llm | parser\n",
        "\n",
        "print(\"LCEL chain (movie_chain) constructed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNkFI0Ii2dUE",
        "outputId": "ef4bfdd8-c743-4456-d118-cc3c169bb79b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LCEL chain (movie_chain) constructed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Now that the LCEL chain has been constructed, invoke it with an example movie name to demonstrate its functionality and confirm it works as expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "QSdfPigp2y5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_chain.invoke({\"movie_name\": \"Inception\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFId38rP2fRH",
        "outputId": "568f4fc7-09f2-40e7-c165-1704bcc1b916"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie \"Inception\" was released in **2010**.\n",
            "\n",
            "Assuming the current year is 2026, that was **16 years ago** (2026 - 2010 = 16).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task was to create a LangChain LCEL chain that takes a `movie_name` as input, queries an LLM to find its release year and calculate the number of years since its release, and then parses the output as a string using existing `llm` and `parser` objects. This task was successfully completed.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A `ChatPromptTemplate` named `movie_prompt_template` was created to instruct the LLM. It includes a `movie_name` placeholder and explicitly asks for the movie's release year and the number of years since its release, assuming the current year is 2026.\n",
        "*   An LCEL chain, `movie_chain`, was successfully constructed by piping together the `movie_prompt_template`, the Language Model (`llm`), and a string output parser (`parser`).\n",
        "*   When invoked with \"Inception\", the `movie_chain` accurately identified the movie's release year as 2010 and calculated that it was released 16 years ago (based on the assumed current year of 2026).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The established LCEL chain is ready for integration into larger applications requiring movie release year information and age calculation.\n",
        "*   Consider enhancing the prompt to dynamically set the current year instead of hardcoding 2026, improving the chain's longevity and accuracy over time.\n"
      ],
      "metadata": {
        "id": "E3dIAo7w2pyH"
      }
    }
  ]
}